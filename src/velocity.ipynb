{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs as s3\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datacube access & sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datacube_instances(points):\n",
    "    \"\"\"Identifies the ITS_LIVE Zarr datacubes corresponding to sampling points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : geopandas.GeoDataFrame\n",
    "        GeoDataFrame containing points along a glacier centreline.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datacubes : list of str\n",
    "        List unique datacube URLs.\n",
    "    points : geopandas.GeoDataFrame\n",
    "        points with corresponding datacube URL appended.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    sample_datacubes\n",
    "    \"\"\"\n",
    "\n",
    "    # Reads ITS_LIVE Zarr datacube catalog as GeoDataFrame\n",
    "    s3fs = s3.S3FileSystem(anon=True)\n",
    "    catalog_url = \"s3://its-live-data/datacubes/catalog_v02.json\"\n",
    "    with s3fs.open(catalog_url, \"r\") as datacube_catalog:\n",
    "        datacube_catalog = json.load(datacube_catalog)\n",
    "        datacube_catalog = gpd.GeoDataFrame.from_features(datacube_catalog, crs=4326)\n",
    "\n",
    "    # Intersects points with datacube outlines\n",
    "    points = (\n",
    "        points.to_crs(4326)\n",
    "        .overlay(datacube_catalog[[\"geometry\", \"zarr_url\"]], how=\"intersection\")\n",
    "        .set_index([\"glacier\", \"cd\"])\n",
    "        .to_crs(3413)\n",
    "    )\n",
    "\n",
    "    # Amends URLs\n",
    "    points[\"zarr_url\"] = (\n",
    "        points[\"zarr_url\"].str.replace(\"http:\", \"s3:\").str.replace(\".s3.amazonaws.com\", \"\")\n",
    "    )\n",
    "\n",
    "    # Creates list of unique datacubes (as their URLs)\n",
    "    datacubes = points[\"zarr_url\"].unique().tolist()\n",
    "    logger.info(f\"Number of unique datacubes: {len(datacubes)}\")\n",
    "\n",
    "    return datacubes, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_datacubes(datacubes, points, spatial_averaging=True):\n",
    "    \"\"\"Samples ITS_LIVE Zarr datacube at points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datacubes : list of str\n",
    "        List unique datacube URLs.\n",
    "    points : geopandas.GeoDataFrame\n",
    "        GeoDataFrame containing points along a glacier centreline.\n",
    "    spatial_averaging : bool\n",
    "        Flag to sample in a 3x3 window around each point [default=True].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : list of xarray.Dataset\n",
    "        Sampled datasets with the dimensions:\n",
    "            - mid_date  ...\n",
    "            - glacier   ...\n",
    "            - cd        binned distance along centreline\n",
    "            - k         int (0-8) each point in the 3x3 kernel\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    get_datacube_instances\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates storage needed for spatial averaging, if necessary\n",
    "    if spatial_averaging:\n",
    "        points[\"windows\"] = pd.Series(dtype=object)\n",
    "\n",
    "    # Creates storage for sampled datasets\n",
    "    datasets = []\n",
    "\n",
    "    for dc_url in datacubes:\n",
    "        # Gets index of points within current the datacube\n",
    "        z_idx = points[\"zarr_url\"] == dc_url\n",
    "\n",
    "        with xr.open_dataset(dc_url, engine=\"zarr\", storage_options={\"anon\": True}) as dc:\n",
    "            if not spatial_averaging:\n",
    "                # Gets point x and y coords\n",
    "                point_x = points.loc[z_idx, \"x\"].to_xarray()\n",
    "                point_y = points.loc[z_idx, \"y\"].to_xarray()\n",
    "                mask = ~np.isnan(point_x)\n",
    "\n",
    "                # Queries datacube at point\n",
    "                dc = dc.sel(x=point_x, y=point_y, method=\"nearest\").sortby(\"mid_date\")\n",
    "\n",
    "            else:\n",
    "                # Creates additional points for 3x3 kernal about each index\n",
    "                diffs_x = np.abs(points.loc[z_idx, \"x\"].values - dc[\"x\"].values[:, np.newaxis])\n",
    "                diffs_y = np.abs(points.loc[z_idx, \"y\"].values - dc[\"y\"].values[:, np.newaxis])\n",
    "                x_idx = diffs_x.argmin(axis=0)\n",
    "                y_idx = diffs_y.argmin(axis=0)\n",
    "                windows = []\n",
    "                for x, y in list(zip(x_idx, y_idx)):\n",
    "                    window = []\n",
    "                    for y_ in [y + 1, y, y - 1]:\n",
    "                        for x_ in [x - 1, x, x + 1]:\n",
    "                            window.append((x_, y_))\n",
    "                    windows.append(window)\n",
    "\n",
    "                # Appends window indicies to points GeoDataFrame\n",
    "                points.loc[z_idx, \"windows\"] = pd.Series(\n",
    "                    windows, index=points.loc[z_idx].index\n",
    "                )\n",
    "\n",
    "                # Unpacks list of windows\n",
    "                points = (\n",
    "                    points.loc[z_idx, \"windows\"]\n",
    "                    .apply(pd.Series)\n",
    "                    .reset_index()\n",
    "                    .melt([\"glacier\", \"cd\"])\n",
    "                    .sort_values(by=[\"glacier\", \"cd\"])\n",
    "                    .rename(columns={\"variable\": \"k\", \"value\": \"idx\"})\n",
    "                    .set_index([\"glacier\", \"cd\", \"k\"])\n",
    "                )\n",
    "\n",
    "                # Unpacks each window's tuple\n",
    "                points = (\n",
    "                    points[\"idx\"].apply(pd.Series).rename(columns={0: \"x_idx\", 1: \"y_idx\"})\n",
    "                )\n",
    "\n",
    "                # Removes invalid indicies caused when points cross a datacube boundary\n",
    "                invalid = (\n",
    "                    (points[\"y_idx\"] >= len(dc[\"y\"]))\n",
    "                    | (points[\"y_idx\"] < 0)\n",
    "                    | (points[\"x_idx\"] >= len(dc[\"x\"]))\n",
    "                    | (points[\"x_idx\"] < 0)\n",
    "                )\n",
    "                points = points[~invalid]\n",
    "\n",
    "                # Gets point x and y coords\n",
    "                point_x = points[\"x_idx\"].to_xarray()\n",
    "                point_y = points[\"y_idx\"].to_xarray()\n",
    "                mask = ~np.isnan(point_x)\n",
    "\n",
    "                # Handles NaN values created when dealing with centrelines of differing length\n",
    "                point_x = xr.where(mask, point_x, 0).astype(int)\n",
    "                point_y = xr.where(mask, point_y, 0).astype(int)\n",
    "\n",
    "                # Queries datacube at point\n",
    "                dc = dc.isel(x=point_x, y=point_y).sortby(\"mid_date\")\n",
    "\n",
    "            # Masks NaN values\n",
    "            dc[\"x\"] = xr.where(mask, dc[\"x\"], np.nan)\n",
    "            dc[\"y\"] = xr.where(mask, dc[\"y\"], np.nan)\n",
    "            dc[\"mask\"] = mask\n",
    "\n",
    "            logger.info(f\"Datacube dimensions: {dc.dims}\")\n",
    "            datasets.append(dc.chunk(chunks={\"cd\": 15, \"glacier\": 1}))\n",
    "\n",
    "    logger.info(f\"Number of datacubes loaded: {len(datasets)}\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error weighting & uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_weighted_average(values, error, resample_frequency=\"AS\"):\n",
    "    \"\"\"Calculates error weighted average.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : xarray.DataArray\n",
    "        Values to average.\n",
    "    error : xarray.DataArray\n",
    "        Error associated with each value.\n",
    "    resample_frequency : str\n",
    "        Resampling frequency.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weighted_average : xarray.DataArray\n",
    "        Error weighted average, resampled to the specified frequency.\n",
    "    combined_uncertainty : xarray.DataArray\n",
    "        Associated uncertainty.\n",
    "    std : xarray.DataArray\n",
    "        Associated standard deviations.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    get_annual_averages\n",
    "    get_seasonal_averages\n",
    "    get_time_series\n",
    "    v_uncertainty\n",
    "    \"\"\"\n",
    "\n",
    "    empty = np.zeros(values.shape)\n",
    "    empty[:] = np.nan\n",
    "\n",
    "    error_masked = xr.where(np.isnan(values), empty, error)\n",
    "    weights = 1 / (error_masked**2)\n",
    "\n",
    "    top = (weights * values).resample(mid_date=resample_frequency).sum()\n",
    "    bottom = weights.resample(mid_date=resample_frequency).sum()\n",
    "    weighted_average = top / bottom\n",
    "\n",
    "    combined_uncertainty = np.sqrt(1 / (weights.resample(mid_date=resample_frequency).sum()))\n",
    "    std = values.resample(mid_date=resample_frequency).std()\n",
    "\n",
    "    return (\n",
    "        weighted_average.rename(\"v\"),\n",
    "        combined_uncertainty.rename(\"v_error\"),\n",
    "        std.rename(\"std\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_uncertainty(vx, vy, vx_error=0, vy_error=0):\n",
    "    \"\"\"Calculates velocity and associated uncertainty from vx and vy components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vx : xarray.DataArray\n",
    "        vx velocity component.\n",
    "    vy : xarray.DataArray\n",
    "        vy velocity component.\n",
    "    vx_error : xarray.DataArray\n",
    "        Error associated with vx velocity component.\n",
    "    vy_error : xarray.DataArray\n",
    "        Error associated with vy velocity component.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v : xarray.DataArray\n",
    "        Resultant velocity, calculated from vx and vy components.\n",
    "    v_error : xarray.DataArray\n",
    "        Associated uncertainty.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    v = sqrt(vx^2 + vy^2)\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    error_weighted_average\n",
    "    get_annual_averages\n",
    "    get_seasonal_averages\n",
    "    get_time_series\n",
    "    \"\"\"\n",
    "\n",
    "    def pow_err(q, x, dx, n):\n",
    "        \"\"\"Returns uncertainty in q (dq), where q = x^n and x has uncertainty dx\n",
    "        dq / |q| = |n|* dx / |x|\n",
    "        \"\"\"\n",
    "        return np.abs(q) * np.abs(n) * (dx / np.abs(x))\n",
    "\n",
    "    vx2_error = pow_err(vx**2, vx, vx_error, 2)  # error in vx^2\n",
    "    vy2_error = pow_err(vy**2, vy, vy_error, 2)  # error in vy^2\n",
    "\n",
    "    sq_sum = vx**2 + vy**2\n",
    "    sq_sum_error = np.hypot(vx2_error, vy2_error)  # error in vx^2 + vy^2\n",
    "\n",
    "    v = np.sqrt(sq_sum)  # resultant velocity\n",
    "    v_error = pow_err(v, sq_sum, sq_sum_error, 0.5)  # error in resultant velocity\n",
    "\n",
    "    return v, v_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annual_averages(datasets, date_dt_min, date_dt_max):\n",
    "    \"\"\"Calculates average annual velocities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list of xarray.DataArray\n",
    "        Sampled IT_LIVE data.\n",
    "    date_dt_min : str\n",
    "        pandas timedelta string specifying minimum date_dt.\n",
    "    date_dt_max : str\n",
    "        pandas timedelta string specifying minimum date_dt.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v_annual : pandas.DataFrame\n",
    "        DataFrame of average annual velocities and their associated error.\n",
    "    v_all : pandas.DataFrame\n",
    "        DataFrame of spatially averaged velocities at *all* timesteps in the original xarray.Dataset (useful\n",
    "        for plotting standard deviations with seaborn using .groupby(['glacier','cd','year]))\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Filters sampled velocity data to only include that derived from image pairs with a\n",
    "    separation time (date_dt) between date_dt_min and date_dt_max.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    error_weighted_average\n",
    "    get_seasonal_averages\n",
    "    get_time_series\n",
    "    v_uncertainty\n",
    "    \"\"\"\n",
    "\n",
    "    annual_v = []\n",
    "    annual_v_error = []\n",
    "\n",
    "    for ds in datasets:\n",
    "        # Filter date_dt between date_dt_min and date_dt_max\n",
    "        date_dt_idx = (ds[\"date_dt\"] >= pd.Timedelta(date_dt_min)) & (\n",
    "            ds[\"date_dt\"] <= pd.Timedelta(date_dt_max)\n",
    "        )\n",
    "\n",
    "        if \"k\" not in list(ds.dims):\n",
    "            # No spatial averaging\n",
    "            v, v_error, _ = error_weighted_average(\n",
    "                ds[\"v\"][date_dt_idx, :, :], ds[\"v_error\"][date_dt_idx, :, :]\n",
    "            )\n",
    "            annual_v.append(v)\n",
    "            annual_v_error.append(v_error)\n",
    "\n",
    "        else:\n",
    "            # Calculates average velocity within each kernal from vx and vy components\n",
    "            average_vx = ds[\"vx\"][date_dt_idx.compute(), :, :, :].mean(dim=\"k\", skipna=True)\n",
    "            average_vx = ds[\"vy\"][date_dt_idx.compute(), :, :, :].mean(dim=\"k\", skipna=True)\n",
    "            average_v, average_v_error = v_uncertainty(\n",
    "                average_vx,\n",
    "                average_vx,\n",
    "                ds[\"vx_error\"][date_dt_idx.compute()],\n",
    "                ds[\"vy_error\"][date_dt_idx.compute()],\n",
    "            )\n",
    "\n",
    "            v, v_error, _ = error_weighted_average(average_v, average_v_error)\n",
    "\n",
    "            output_shape = dict(zip(v.dims, v.shape))\n",
    "            valid_idx = np.repeat(\n",
    "                (ds[\"mask\"].mean(dim=\"k\").values == 1)[np.newaxis, :, :],\n",
    "                repeats=output_shape[\"mid_date\"],\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "            v = xr.where(valid_idx, v, np.nan)\n",
    "            v_error = xr.where(valid_idx, v_error, np.nan)\n",
    "            annual_v.append(v)\n",
    "            annual_v_error.append(v_error)\n",
    "\n",
    "    # Merge annual datasets\n",
    "    v = xr.merge(annual_v)[\"v\"]\n",
    "    v_error = xr.merge(annual_v_error)[\"v_error\"]\n",
    "\n",
    "    # Convert to DataFrame for better control over plotting\n",
    "    v_error_annual = v_error.to_dataframe(\n",
    "        name=\"v_error\", dim_order=[\"glacier\", \"cd\", \"mid_date\"]\n",
    "    )\n",
    "    v_annual = v.to_dataframe(name=\"v\", dim_order=[\"glacier\", \"cd\", \"mid_date\"])\n",
    "    v_annual = v_annual.merge(v_error_annual, left_index=True, right_index=True)\n",
    "\n",
    "    v_annual[\"year\"] = v_annual.index.get_level_values(level=2).year\n",
    "    v_annual[\"pcnt\"] = (\n",
    "        v_annual[\"v\"]\n",
    "        / v_annual.loc[\n",
    "            v_annual.groupby([\"glacier\", \"cd\"])[\"year\"].transform(\"idxmin\"), \"v\"\n",
    "        ].values\n",
    "    ) * 100\n",
    "    v_annual.reset_index(inplace=True)\n",
    "\n",
    "    v_all = average_v.to_dataframe(name=\"v\", dim_order=[\"glacier\", \"cd\", \"mid_date\"])\n",
    "    v_all[\"year\"] = v_all.index.get_level_values(level=2).year\n",
    "    v_all.reset_index(inplace=True)\n",
    "\n",
    "    return v_annual, v_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeasonalAverages(dss, startD, stopD, ddt_max=\"30D\", rsmpFreq=\"Q-FEB\"):\n",
    "    \"\"\"\n",
    "    get seasonally averaged velocities\n",
    "    input:  dss - list of xr.Datasets dims = [mid_date, glacier, cd, (k)]\n",
    "            startD, stopD - pandas datetimes for filtering on mid_date\n",
    "            ddt_max - pandas dateoffset string for filtering date_dt - default 30 days\n",
    "            rsmpFreq - frequnecy to resample to, default = Q-FEB\n",
    "    returns: sv_df : a pandas data frame with columns:\n",
    "                glacier, cd, mid_date (with frequency == rsmpFreq), v, season, year\n",
    "    \"\"\"\n",
    "\n",
    "    seasonalVs = []\n",
    "    # seasonalErrs = []\n",
    "    # seasonalStds = []\n",
    "    for ds in dss:\n",
    "        # date range\n",
    "        d_idx = ds[\"mid_date\"].to_pandas().between(startD, stopD)\n",
    "        # date_dt filter\n",
    "        ddt_idx = ds[\"date_dt\"] <= pd.Timedelta(ddt_max)\n",
    "        # combined mid_date filter\n",
    "        mid_date_idx = ddt_idx & d_idx\n",
    "\n",
    "        if \"k\" in list(ds.dims):  # spatialAveraging: if spatialAveraging\n",
    "            # compute average velocity in each kernal, from vx,vy components\n",
    "            avg_vx = ds[\"vx\"][mid_date_idx, :, :, :].mean(dim=\"k\", skipna=True)\n",
    "            avg_vy = ds[\"vy\"][mid_date_idx, :, :, :].mean(dim=\"k\", skipna=True)\n",
    "            avg_v, avg_v_err = v_uncert(\n",
    "                avg_vx, avg_vy, ds[\"vx_error\"][mid_date_idx], ds[\"vy_error\"][mid_date_idx]\n",
    "            )\n",
    "            # # or just do ds['v'].mean(dim = 'k')...and use the supplied 'v_error'\n",
    "            seasonalV, _, _ = errWgtAvg(avg_v, avg_v_err, freq=rsmpFreq)\n",
    "\n",
    "            output_shape = dict(zip(seasonalV.dims, seasonalV.shape))\n",
    "            valid_idx = np.repeat(\n",
    "                (ds[\"mask\"].mean(dim=\"k\").values == 1)[np.newaxis, :, :],\n",
    "                repeats=output_shape[\"mid_date\"],\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "            seasonalV = xr.where(valid_idx, seasonalV, np.nan)\n",
    "            # seasonalErr = xr.where(valid_idx, seasonalErr, np.nan)\n",
    "            # seasonalStd = xr.where(valid_idx, seasonalStd, np.nan)\n",
    "\n",
    "            seasonalVs.append(seasonalV)\n",
    "            # seasonalErrs.append(seasonalErr)\n",
    "            # seasonalStds.append(seasonalStd)\n",
    "\n",
    "        else:\n",
    "            avg_v = ds[\"v\"][mid_date_idx, :, :]\n",
    "            seasonalV, _, _ = errWgtAvg(\n",
    "                ds[\"v\"][mid_date_idx, :, :], ds[\"v_error\"][mid_date_idx, :, :]\n",
    "            )\n",
    "            # seasonalVs.append(seasonalV)\n",
    "            # seasonalErrs.append(seasonalErr)\n",
    "            seasonalVs.append(seasonalV)\n",
    "\n",
    "    Sv = xr.merge(seasonalVs)[\"v\"]\n",
    "\n",
    "    ## seasonal trends along centreline constructed by first calculating seasonal averages (1 per year)\n",
    "    # then computing trend\n",
    "    ssn = {1: \"DJF\", 2: \"MAM\", 3: \"JJA\", 4: \"SON\"}\n",
    "    sv_df = Sv.to_dataframe(dim_order=[\"glacier\", \"cd\", \"mid_date\"])\n",
    "    sv_df.reset_index(inplace=True)\n",
    "    sv_df[\"season\"] = (sv_df[\"mid_date\"].dt.month % 12 // 3 + 1).map(ssn)\n",
    "    sv_df[\"year\"] = sv_df[\"mid_date\"].dt.year\n",
    "\n",
    "    return sv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeseries(dss, startD, stopD, distances, ddt_max=\"30D\"):\n",
    "    \"\"\"\n",
    "    for formatting timeseries at a point(s) along centreline, and resampling them to regular frequnecies\n",
    "    inputs: dss - list of xarray datasets, dims(mid_date, glacier, cd, (k))\n",
    "            startD, stopD - pandas datetimes for filtering - mid_date\n",
    "            distances - list of distances along centreline (cd) to include\n",
    "            ddt_max - pandas date offset string (e.g. '30D'. for filtering date_dt. only include date_dts <= ddt_max\n",
    "\n",
    "    returns: ts_df:\n",
    "        pandas dataframe: velocities that meet date, date_dt and cd filter requirements (spatially averaged if `k` is present in list of dims)\n",
    "        w/  rolling 28 d average\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for ds in dss:\n",
    "        # filter on startD, stopD and ddt_max\n",
    "        d_idx = ds[\"mid_date\"].to_pandas().between(startD, stopD)\n",
    "        ddt_idx = ds[\"date_dt\"] <= pd.Timedelta(ddt_max)\n",
    "        mid_date_idx = ddt_idx & d_idx\n",
    "\n",
    "        ## filter on distances\n",
    "        cd_idx = ds[\"cd\"].isin(distances)\n",
    "\n",
    "        if \"k\" in list(ds.dims):  # then spatial averaging\n",
    "            # average spatially in kernal.\n",
    "            avg_vx = ds[\"vx\"][mid_date_idx, :, cd_idx, :].mean(dim=\"k\", skipna=True)\n",
    "            avg_vy = ds[\"vy\"][mid_date_idx, :, cd_idx, :].mean(dim=\"k\", skipna=True)\n",
    "            avg_v, avg_v_err = v_uncert(\n",
    "                avg_vx, avg_vy, ds[\"vx_error\"][mid_date_idx], ds[\"vy_error\"][mid_date_idx]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            avg_v, avg_v_err = v_uncert(\n",
    "                ds[\"vx\"][mid_date_idx, :, cd_idx],\n",
    "                ds[\"vy\"][mid_date_idx, :, cd_idx],\n",
    "                ds[\"vx_error\"][mid_date_idx],\n",
    "                ds[\"vy_error\"][mid_date_idx],\n",
    "            )\n",
    "\n",
    "        # merge v, v_err and meta data\n",
    "        filtered.append(\n",
    "            xr.merge(\n",
    "                [\n",
    "                    avg_v.rename(\"v\"),\n",
    "                    avg_v_err.rename(\"err\"),\n",
    "                    ds[\n",
    "                        [\n",
    "                            \"acquisition_date_img1\",\n",
    "                            \"acquisition_date_img2\",\n",
    "                            \"date_dt\",\n",
    "                            \"satellite_img1\",\n",
    "                            \"mission_img1\",\n",
    "                        ]\n",
    "                    ].sel(mid_date=mid_date_idx),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # merge list of filterd dataframes and convert to dask_df\n",
    "    ds_out = xr.merge(filtered)\n",
    "    ts_df = ds_out.to_dataframe()\n",
    "\n",
    "    # # drop nan rows\n",
    "    ts_df = ts_df[~ts_df[\"v\"].isna()]\n",
    "\n",
    "    # rolling median\n",
    "    ts_df = ts_df.merge(\n",
    "        ts_df.reset_index(level=[1, 2])\n",
    "        .groupby([\"glacier\", \"cd\"])[\"v\"]\n",
    "        .rolling(\"28D\")\n",
    "        .median()\n",
    "        .rename(\"smoothed_v\")\n",
    "        .reset_index(),\n",
    "        left_on=[\"mid_date\", \"glacier\", \"cd\"],\n",
    "        right_on=[\"mid_date\", \"glacier\", \"cd\"],\n",
    "    )\n",
    "\n",
    "    # calculate intermediate steps for error-weighted averages\n",
    "    ts_df[\"v_over_sigma2\"] = ts_df[\"v\"] / (ts_df[\"err\"] ** 2)\n",
    "    ts_df[\"one_over_sigma2\"] = 1 / (ts_df[\"err\"] ** 2)\n",
    "\n",
    "    return ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dask.distributed import Client\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines plot settings\n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"stix\"\n",
    "plt.rcParams[\"xtick.labelsize\"] = 11\n",
    "plt.rcParams[\"ytick.labelsize\"] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "INFO:distributed.scheduler:State start\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\olive\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\scheduler-b6p2inuk', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\olive\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\scheduler-kwgaa1eb', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\olive\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\scheduler-t7zmoy4h', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\olive\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\worker-8vi7rv52', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\olive\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\worker-hm3x32i8', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\olive\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\worker-ooirmr8k', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\olive\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\worker-yxs3ogp5', purging\n",
      "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:55156\n",
      "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
      "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:55165'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:55161'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:55163'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:55159'\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:55175', name: 1, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:55175\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55177\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:55178', name: 2, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:55178\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55183\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:55179', name: 3, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:55179\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55182\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:55184', name: 0, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:55184\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55186\n",
      "INFO:distributed.scheduler:Receive client connection: Client-36f684e5-070f-11ef-9354-9c2dcd391931\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55187\n",
      "INFO:__main__:Dask dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "dask_client = Client()\n",
    "logger.info(f\"Dask dashboard: {dask_client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads sampling points\n",
    "points_path = \"C:\\\\Users\\\\olive\\\\Projects\\\\diss\\\\src\\\\holt_isortuarsuup-sermia_10-5281-zendono-7824987\\\\tlohde-isortuarsuupSermia-396b33c\\\\data\\\\samplePoints_feather\"\n",
    "points = gpd.read_feather(points_path)\n",
    "\n",
    "selection = [\"IsortuarsuupSermia\", \"KangaasarsuupSermia\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample datacubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Number of unique datacubes: 1\n",
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_21332\\3837882313.py:70: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  .apply(pd.Series)\n",
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_21332\\3837882313.py:80: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  points[\"idx\"].apply(pd.Series).rename(columns={0: \"x_idx\", 1: \"y_idx\"})\n",
      "INFO:__main__:Datacube dimensions: Frozen({'mid_date': 42808, 'glacier': 2, 'cd': 69, 'k': 9})\n",
      "INFO:__main__:Number of datacubes loaded: 1\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED RUNTIME: ~3 minutes\n",
    "\n",
    "# Identifies and sample ITS_LIVE Zarr datacubes that intersect with sampling points\n",
    "datacubes, points = get_datacube_instances(points)\n",
    "datasets = sample_datacubes(datacubes, points, spatial_averaging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Roaming\\Python\\Python311\\site-packages\\distributed\\client.py:3169: UserWarning: Sending large graph of size 1.20 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "C:\\Users\\olive\\AppData\\Roaming\\Python\\Python311\\site-packages\\distributed\\client.py:3169: UserWarning: Sending large graph of size 1.20 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "C:\\Users\\olive\\AppData\\Roaming\\Python\\Python311\\site-packages\\distributed\\client.py:3169: UserWarning: Sending large graph of size 1.19 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED RUNTIME: ~2 minutes\n",
    "\n",
    "# Calculates annual averages\n",
    "date_dt_min = \"300D\"  # 300 days\n",
    "date_dt_max = \"430D\"  # 430 days\n",
    "v_annual, v_all = get_annual_averages(datasets, date_dt_min, date_dt_max)\n",
    "\n",
    "# Calculates annual trends\n",
    "annual_trend = (\n",
    "    v_annual.groupby([\"glacier\", \"cd\"])\n",
    "    .apply(lambda q: pd.Series(linregress(x=q[\"year\"], y=q[\"v\"])))\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"slope\", 1: \"intercept\", 2: \"rvalue\", 3: \"pvalue\", 4: \"stderr\"})\n",
    ")\n",
    "\n",
    "# Converts from metres to kilometres\n",
    "v_all[\"cd_km\"] = v_all[\"cd\"] / 1000\n",
    "v_annual[\"cd_km\"] = v_annual[\"cd\"] / 1000\n",
    "annual_trend[\"cd_km\"] = annual_trend[\"cd\"] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual velocities, percentage, and trends\n",
    "fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(10, 8), sharex=True, sharey=\"row\")\n",
    "\n",
    "for i, g in enumerate(selection):\n",
    "    # annual velocity with standard deviation as measure of spread\n",
    "    idx = (v_all[\"glacier\"] == g) & (v_all[\"year\"] < 2022)\n",
    "    if i == 1:\n",
    "        lgnd = \"full\"\n",
    "    else:\n",
    "        lgnd = False\n",
    "    sns.lineplot(\n",
    "        data=v_all.loc[idx, :],\n",
    "        x=\"cd_km\",\n",
    "        y=\"v\",\n",
    "        errorbar=\"sd\",  # with standard deviation as measure of spread\n",
    "        hue=\"year\",\n",
    "        style=\"year\",\n",
    "        ax=axs[0, i],\n",
    "        legend=lgnd,\n",
    "    )\n",
    "    axs[1, i].set_ylim(top=251)\n",
    "    axs[0, i].set_title(g)\n",
    "\n",
    "    # percentage change\n",
    "    idx = (v_annual[\"glacier\"] == g) & (v_annual[\"year\"] < 2022)\n",
    "    sns.lineplot(\n",
    "        data=df.loc[idx],\n",
    "        x=\"cd_km\",\n",
    "        y=\"pcnt\",\n",
    "        hue=\"year\",\n",
    "        style=\"year\",\n",
    "        ax=axs[1, i],\n",
    "        legend=False,\n",
    "    )\n",
    "    axs[1, i].set_ylim(75, 280)\n",
    "\n",
    "    # acceleration - significant only\n",
    "    idx = (annual_trend[\"glacier\"] == g) & (annual_trend[\"pvalue\"] <= 0.05)\n",
    "    axs[2, i].errorbar(\n",
    "        x=annual_trend.loc[idx, \"cd_km\"],\n",
    "        y=annual_trend.loc[idx, \"slope\"],\n",
    "        yerr=annual_trend.loc[idx, \"stderr\"],\n",
    "        ecolor=\"grey\",\n",
    "        fmt=\".\",\n",
    "        color=\"k\",\n",
    "    )\n",
    "    axs[2, i].set_ylim(-2, 21)\n",
    "\n",
    "# label axes\n",
    "labs = \"abcdefghi\"\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.set_xlim(-0.150, 16.150)\n",
    "    ax.annotate(f\"({labs[i]})\", xy=(0.9, 0.87), xycoords=\"axes fraction\", fontsize=18)\n",
    "    sns.despine(ax=ax, trim=True)\n",
    "\n",
    "###### add inset to KS % velocity plot\n",
    "axins = axs[1, 1].inset_axes([0.12, 0.46, 0.75, 0.48])\n",
    "idx = (v_annual[\"glacier\"] == \"KangaasarsuupSermia\") & (v_annual[\"year\"] < 2022)\n",
    "sns.lineplot(\n",
    "    data=v_annual.loc[idx],\n",
    "    x=\"cd_km\",\n",
    "    y=\"pcnt\",\n",
    "    hue=\"year\",\n",
    "    style=\"year\",\n",
    "    ax=axins,\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# sub region of the original image\n",
    "x1, x2, y1, y2 = 2.5, 8, 95, 140\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "axins.set_xticklabels([])\n",
    "axins.set_yticks(axins.get_yticks()[1:3])\n",
    "axins.set_ylabel(None)\n",
    "axins.set_xlabel(None)\n",
    "axins.set_yticklabels(\n",
    "    [int(q) for q in axins.get_yticks()], rotation=90, ha=\"center\", va=\"center\"\n",
    ")\n",
    "axs[1, 1].indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "\n",
    "# label axes\n",
    "axs[0, 0].set_ylabel(\"Velocity (m yr$^{-1}$)\")\n",
    "axs[1, 0].set_ylabel(\"Velocity\\nchange (%)\")\n",
    "axs[2, 0].set_ylabel(\"Velocity\\ntrend (m yr$^{-2}$)\")\n",
    "axs[2, 0].set_xlabel(\"Distance from terminus (km)\")\n",
    "axs[2, 1].set_xlabel(\"Distance form terminus (km)\")\n",
    "\n",
    "# tidy up legend\n",
    "sns.move_legend(\n",
    "    axs[0, 1],\n",
    "    loc=\"upper left\",\n",
    "    bbox_to_anchor=(0, 1.05),\n",
    "    ncol=3,\n",
    "    fontsize=12,\n",
    "    title=\"Year\",\n",
    "    labelspacing=0.1,\n",
    "    columnspacing=0.5,\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "leg = axs[0, 1].get_legend()\n",
    "for legobj in leg.legend_handles:\n",
    "    legobj.set_linewidth(2)\n",
    "plt.subplots_adjust(hspace=0.08, wspace=0.05)\n",
    "\n",
    "# fig.savefig(\"../figures/f02.png\", bbox_inches=\"tight\")\n",
    "\n",
    "print(\n",
    "    \"Figure 2: Annual average ice velocity (2013–2021) profiles along centrelines shown in Figure 1 at\\n\\\n",
    "    (a) Isortuarsuup Sermia, and (b) Kangaasarsuup Sermia. (c) and (d) show percentage change relative to 2013. \\n\\\n",
    "        (e) and (f) show linear trends where regression slope coefficients are significant at p ≤ .05; \\n\\\n",
    "            error bars denote standard error of the estimate.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~ 40 seconds\n",
    "startD = pd.to_datetime(\"2016-01-01\")\n",
    "stopD = pd.to_datetime(\"2022-01-01\")\n",
    "# distances = np.arange(500,10500,500)\n",
    "distances = [1000, 10000]\n",
    "ddt_max = \"30D\"\n",
    "\n",
    "# get timeseries timeseries at each point/glacier\n",
    "ts_df = h.getTimeseries(dss, startD, stopD, distances, ddt_max)\n",
    "ts_df[\"day_sep\"] = ts_df[\"date_dt\"] / pd.Timedelta(\"1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting with simple median filter\n",
    "fg = sns.relplot(\n",
    "    data=(\n",
    "        ts_df.set_index(\"mid_date\")\n",
    "        .groupby([\"glacier\", \"cd\"])[\"v\"]\n",
    "        .rolling(\"28D\", min_periods=10, center=True)\n",
    "        .median()\n",
    "        .rename(\"smoothed_v\")\n",
    "        .reset_index()\n",
    "    ),\n",
    "    x=\"mid_date\",\n",
    "    y=\"smoothed_v\",\n",
    "    hue=\"glacier\",\n",
    "    palette=\"colorblind\",\n",
    "    style=\"glacier\",\n",
    "    linewidth=2,\n",
    "    row=\"cd\",\n",
    "    kind=\"line\",\n",
    "    aspect=1.8,\n",
    ")\n",
    "\n",
    "for d in distances:\n",
    "    tsdf_idx = ts_df[\"cd\"] == d\n",
    "    sns.scatterplot(\n",
    "        data=ts_df.loc[tsdf_idx],\n",
    "        x=\"mid_date\",\n",
    "        y=\"v\",\n",
    "        hue=\"glacier\",\n",
    "        palette=\"colorblind\",\n",
    "        style=\"glacier\",\n",
    "        alpha=0.8,\n",
    "        s=2,\n",
    "        ax=fg.axes_dict[d],\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "sns.move_legend(fg, loc=\"upper right\", bbox_to_anchor=(0.7, 0.5))  # (0.75, 0.53)\n",
    "\n",
    "fg.set_titles(\"\")\n",
    "\n",
    "labs = \"abcdefg\"\n",
    "for i, ax in enumerate(fg.axes.flatten()):\n",
    "    ax.annotate(f\"({labs[i]})\", xy=(0.05, 0.88), xycoords=\"axes fraction\", fontsize=18)\n",
    "    ax.annotate(\n",
    "        f\"{str(int(distances[i]/1000))} km from terminus\",\n",
    "        xy=(0.1, 0.88),\n",
    "        xycoords=\"axes fraction\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    # ax.set_title(f'Distance from terminus: {str(int(distances[i]/1000))} km', y=0.82, loc='left')\n",
    "\n",
    "fg.set(ylim=(-10, 340))\n",
    "fg.set_xticklabels([2016, 2017, 2018, 2019, 2020, 2021, 2022])\n",
    "fg.set(ylabel=(\"Velocity (m yr$^{-1}$)\"), xlabel=(\"Year\"))\n",
    "fg.despine(trim=True)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.01)\n",
    "\n",
    "# fg.fig.savefig('../figures/f03.png',\n",
    "#                bbox_inches='tight')\n",
    "\n",
    "print(\n",
    "    \"Figure 3: Time series of ice surface velocity at (a) 1 km and (b) 10 km from the terminus at\\n\\\n",
    "      the lake-terminating Isortuarsuup Sermia (blue) and\\n\\\n",
    "            the land-terminating Kangaasarsuup Sermia (orange, dashed). \\n\\\n",
    "      These velocities are computed from image-pairs separated by ≤ 30 days.\\n\\\n",
    "            Lines show the rolling 28 day median.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~40 seconds\n",
    "##### compute seasonal averages and their trends along glacier centrelines\n",
    "startD = pd.to_datetime(\"2016-01-01\")\n",
    "stopD = pd.to_datetime(\"2022-01-01\")\n",
    "ddt_max = \"30D\"\n",
    "rsmpFreq = \"Q-FEB\"\n",
    "\n",
    "sv_df = h.getSeasonalAverages(dss, startD, stopD, ddt_max, rsmpFreq)\n",
    "\n",
    "## compute seasonal trends\n",
    "ssn_trend = (\n",
    "    sv_df.groupby([\"glacier\", \"season\", \"cd\"])\n",
    "    .apply(lambda q: pd.Series(linregress(x=q[\"year\"], y=q[\"v\"])))\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"slope\", 1: \"intercept\", 2: \"rvalue\", 3: \"pvalue\", 4: \"stderr\"})\n",
    ")\n",
    "\n",
    "ssn_trend[\"significant\\np<0.05\"] = ssn_trend[\"pvalue\"] <= 0.05\n",
    "ssn_trend[\"Distance (km)\"] = ssn_trend[\"cd\"] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color palette. from PuOr from colorbrewer. should be BW and colorblind safe\n",
    "rgbs = [(94, 60, 153), (253, 184, 99), (230, 97, 1), (178, 171, 210)]\n",
    "rgbs = [tuple(i / 255 for i in a) for a in rgbs]\n",
    "season_colors = dict(zip([\"DJF\", \"MAM\", \"JJA\", \"SON\"], rgbs))\n",
    "\n",
    "## plot seasonal trends - color and style by season. size by significance\n",
    "fg = sns.relplot(\n",
    "    data=ssn_trend,\n",
    "    x=\"Distance (km)\",\n",
    "    y=\"slope\",\n",
    "    hue=\"season\",\n",
    "    hue_order=[\"DJF\", \"MAM\", \"JJA\", \"SON\"],\n",
    "    palette=rgbs,\n",
    "    style=\"season\",\n",
    "    style_order=[\"DJF\", \"MAM\", \"JJA\", \"SON\"],\n",
    "    markers=[\"o\", \"d\", \"^\", \"P\"],\n",
    "    size=\"significant\\np<0.05\",\n",
    "    size_order=[True, False],\n",
    "    col=\"glacier\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "fg.map(plt.axhline, y=0, color=\"grey\", zorder=0.5)\n",
    "fg.set(xlim=(-0.5, 16), ylim=(-15.5, 26.5), yticks=np.arange(-15, 30, 5))\n",
    "fg.set_ylabels(\"Velocity trend (m yr$^{-2}$)\")\n",
    "fg.despine(trim=True)\n",
    "\n",
    "# label axes\n",
    "labs = \"abcdef\"\n",
    "for i, ax in enumerate(fg.axes.flat):\n",
    "    ax.annotate(f\"({labs[i]})\", xy=(0.05, 0.94), xycoords=\"axes fraction\", fontsize=18)\n",
    "    t = ax.get_title()\n",
    "    ax.set_title(t.split(\"glacier = \")[1], y=0.92)\n",
    "    ax.set_xlabel(\"Distance from terminus (km)\")\n",
    "\n",
    "# error bars\n",
    "for g in selection:\n",
    "    for S in [\"DJF\", \"MAM\", \"JJA\", \"SON\"]:\n",
    "        idx = (\n",
    "            (ssn_trend[\"glacier\"] == g)\n",
    "            & (ssn_trend[\"season\"] == S)\n",
    "            & (ssn_trend[\"significant\\np<0.05\"] == True)\n",
    "        )\n",
    "        fg.axes_dict[g].errorbar(\n",
    "            x=ssn_trend.loc[idx, \"Distance (km)\"],\n",
    "            y=ssn_trend.loc[idx, \"slope\"],\n",
    "            yerr=ssn_trend.loc[idx, \"stderr\"],\n",
    "            fmt=\"None\",\n",
    "            zorder=0.5,\n",
    "            alpha=0.3,\n",
    "            ecolor=season_colors[S],\n",
    "        )\n",
    "\n",
    "# ridiculous routine. make and plot some dummy data in order to construct two separate legends\n",
    "# because just using the `ncols` paramter splits the seasons over two columns, which is ugly\n",
    "dummy = pd.DataFrame(\n",
    "    {\n",
    "        \"a\": [-5, -5, -5, -5],\n",
    "        \"b\": [0, 0, 0, 0],\n",
    "        \"season\": [\"Winter (DJF)\", \"Spring (MAM)\", \"Summer (JJA)\", \"Autumn (SON)\"],\n",
    "    }\n",
    ")\n",
    "q = sns.scatterplot(\n",
    "    data=dummy,\n",
    "    x=\"a\",\n",
    "    y=\"b\",\n",
    "    hue=\"season\",\n",
    "    hue_order=[\"Winter (DJF)\", \"Spring (MAM)\", \"Summer (JJA)\", \"Autumn (SON)\"],\n",
    "    palette=rgbs,\n",
    "    style=\"season\",\n",
    "    style_order=[\"Winter (DJF)\", \"Spring (MAM)\", \"Summer (JJA)\", \"Autumn (SON)\"],\n",
    "    markers=[\"o\", \"d\", \"^\", \"P\"],\n",
    "    ax=fg.axes[0][0],\n",
    "    legend=True,\n",
    ")\n",
    "sns.move_legend(q, loc=\"upper left\", bbox_to_anchor=(1.05, 0.94), frameon=False)\n",
    "\n",
    "dummy = pd.DataFrame({\"a\": [-5, -5], \"b\": [0, 0], \"significant\\np<0.05\": [True, False]})\n",
    "r = sns.scatterplot(\n",
    "    data=dummy,\n",
    "    x=\"a\",\n",
    "    y=\"b\",  # palette=[rgbs[0],rbgs[2]],\n",
    "    size=\"significant\\np<0.05\",\n",
    "    size_order=[True, False],\n",
    "    ax=fg.axes[0][1],\n",
    ")\n",
    "\n",
    "sns.move_legend(r, loc=\"upper left\", bbox_to_anchor=(0.4, 0.94), frameon=False)\n",
    "fg.axes[0][0].set_zorder(2)\n",
    "\n",
    "# fg.fig.savefig('../figures/f04.png', bbox_inches='tight')\n",
    "\n",
    "print(\n",
    "    \"Figure 4: Seasonal velocity trends (2016–2021) along glacier centrelines at (A) Isortuarsuup Sermia and (B) Kangaasarsuup Sermia for\\n\\\n",
    "    winter (DJF, purple circles), spring (MAM, gold diamonds), summer (JJA, orange triangles) and\\n\\\n",
    "        autumn (SON, grey crosses). Seasonal trends derived from velocity fields were “date_dt” ≤ 30 days. \\n\\\n",
    "            Trends shown are linear fits. Significant (p ≤ .05) trends shown by larger symbols, otherwise smaller symbols;\\n\\\n",
    "                error bars denote standard error of the estimate.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
